{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TK9bWX1V-9Bl"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "OxShR5af_I-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load WikiText-2\n",
        "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
        "\n",
        "# Convert each split's 'text' column to a Python list\n",
        "train_text = dataset['train']['text']\n",
        "valid_text = dataset['validation']['text']\n",
        "test_text  = dataset['test']['text']\n",
        "\n",
        "# Join them into one big string\n",
        "text_data = \"\\n\".join(list(train_text) + list(valid_text) + list(test_text))"
      ],
      "metadata": {
        "id": "JMZciO_9_JoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Total characters:\", len(text_data))\n",
        "print(text_data[:500])  # Preview"
      ],
      "metadata": {
        "id": "ECN7KYxUjiqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "metadata": {
        "id": "c-6fI-YgjDRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_VOCAB = 20000   # limit vocab size\n",
        "MAX_LEN = 100       # max sequence length\n",
        "EMB_DIM = 50        # smaller embedding\n",
        "LSTM_UNITS = 64     # smaller LSTM\n",
        "\n",
        "lines = text_data.split('\\n')\n",
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_VOCAB, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(lines)"
      ],
      "metadata": {
        "id": "sI7xM8Q-iqLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenizer.word_index) #unique words"
      ],
      "metadata": {
        "id": "djouEvq7_V7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Create sequences\n",
        "input_sequences = []\n",
        "for sentence in text_data.split('\\n'):\n",
        "    tokenized_sentence = tokenizer.texts_to_sequences([sentence])[0]\n",
        "    for i in range(1, len(tokenized_sentence)):\n",
        "        input_sequences.append(tokenized_sentence[:i+1])"
      ],
      "metadata": {
        "id": "6F0ZqN2p_Yfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Pad sequences\n",
        "input_sequences = pad_sequences(input_sequences, maxlen=MAX_LEN, padding='pre')\n",
        "\n",
        "# Convert to numpy array\n",
        "input_sequences = np.array(input_sequences)\n",
        "\n",
        "# Split into features (X) and label (y)\n",
        "X = input_sequences[:, :-1]\n",
        "y = input_sequences[:, -1]\n"
      ],
      "metadata": {
        "id": "1QodWS-0khcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = min(MAX_VOCAB, len(tokenizer.word_index) + 1)"
      ],
      "metadata": {
        "id": "6u8mjl_e_oCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape of X:\", X.shape)\n",
        "print(\"Shape of y:\", y.shape)\n",
        "print(\"Vocab size:\", vocab_size)"
      ],
      "metadata": {
        "id": "n_x6zd59_qP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM , Dense\n",
        "from tensorflow.keras.layers import Input"
      ],
      "metadata": {
        "id": "szeORzQj_sqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Input(shape=(MAX_LEN - 1,)))\n",
        "model.add(Embedding(vocab_size, EMB_DIM))\n",
        "model.add(LSTM(LSTM_UNITS))\n",
        "model.add(Dense(vocab_size, activation='softmax'))"
      ],
      "metadata": {
        "id": "CsbCgeJaAMPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "gi9p23MEC1z2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "nNsdn4FKDRw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X, y, epochs=10, validation_split=0.2)"
      ],
      "metadata": {
        "id": "_7XLNyDpDwT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "text = 'Valkyria'\n",
        "\n",
        "for i in range(15):\n",
        "  #tokenize\n",
        "  tokenized_text = tokenizer.texts_to_sequences([text])[0]\n",
        "\n",
        "  #padding\n",
        "  padded_token_text = pad_sequences([tokenized_text], maxlen=MAX_LEN-1, padding='pre')\n",
        "\n",
        "  #predict\n",
        "  predicted_id = np.argmax(model.predict(padded_token_text), axis=-1)[0]\n",
        "\n",
        "  # Map ID to word\n",
        "  predicted_word = tokenizer.index_word.get(predicted_id, '')\n",
        "\n",
        "  # Append predicted word to the text\n",
        "  if predicted_word:\n",
        "      text = text + \" \" + predicted_word\n",
        "      print(text)\n",
        "      time.sleep(0.5)\n",
        "  else:\n",
        "      break # stop if no prediction found\n",
        "\n",
        "print(\"\\nFinal Generated text:\" , text)\n",
        ""
      ],
      "metadata": {
        "id": "8Z8tlVm2m5TJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "871Za4ewo2hl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loss Vs Epochs\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss', linestyle='--')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss vs. Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "V9X591Uac7aC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Get top 5 predicted words\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(top_words, top_probs)\n",
        "plt.xlabel('Predicted Words')\n",
        "plt.ylabel('Probability')\n",
        "plt.title(f\"Top Predictions for: '{text}'\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eaFRToNzdJ2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.style.use('seaborn-v0_8-darkgrid')  # Elegant theme\n",
        "\n",
        "# Training history plot\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(history.history['loss'], marker='o', color='#1f77b4', label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], marker='s', color='#ff7f0e', label='Validation Loss', linestyle='--')\n",
        "\n",
        "plt.xlabel('Epochs', fontsize=12, fontweight='bold')\n",
        "plt.ylabel('Loss', fontsize=12, fontweight='bold')\n",
        "plt.title('ðŸ“‰ Model Training Progress: Loss vs Epochs', fontsize=14, fontweight='bold')\n",
        "plt.legend(frameon=True, fontsize=10)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "l8aqnRJGeSqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot\n",
        "plt.figure(figsize=(8, 5))\n",
        "bars = plt.barh(top_words, top_probs, color=plt.cm.viridis(np.linspace(0.2, 0.8, len(top_words))))\n",
        "plt.xlabel('Probability', fontsize=12, fontweight='bold')\n",
        "plt.ylabel('Predicted Words', fontsize=12, fontweight='bold')\n",
        "plt.title(f'ðŸ”® Top Predictions for: \"{text}\"', fontsize=14, fontweight='bold')\n",
        "plt.gca().invert_yaxis()\n",
        "\n",
        "# Add probability annotations\n",
        "for bar, prob in zip(bars, top_probs):\n",
        "    plt.text(prob + 0.005, bar.get_y() + bar.get_height()/2, f\"{prob:.2f}\", va='center', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "c-6Pgd6AeTTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jZ8md145ekqo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}